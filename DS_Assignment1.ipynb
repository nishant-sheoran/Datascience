{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishant-sheoran/Datascience/blob/main/DS_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment by:RA2211003011324 & RA2211003011325"
      ],
      "metadata": {
        "id": "4geqSiWOeV5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Beautiful Soup4 + JS"
      ],
      "metadata": {
        "id": "xGNHQ_y7iQXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y chromium-chromedriver\n",
        "!pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sKTisi0j-0U",
        "outputId": "10cdb720-4462-47b1-c85e-ada074bf5e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 48 not upgraded.\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.29.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.20)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.29.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y chromium-chromedriver\n",
        "!pip install selenium\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxAi0oDtkCaQ",
        "outputId": "485395c6-90df-48e4-abe6-e57a6dd50816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 48 not upgraded.\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.29.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.20)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.29.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6dbIbzx_kOVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Configure Selenium for JS-rendered pages\n",
        "def setup_driver():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    chrome_options.binary_location = \"/usr/bin/chromium-browser\"  # Correct binary location\n",
        "\n",
        "    try:\n",
        "        driver = webdriver.Chrome(options=chrome_options)\n",
        "        return driver\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up Selenium WebDriver: {e}\")\n",
        "        return None\n",
        "\n",
        "def scrape_articles(base_url, num_pages=5):\n",
        "    articles = []\n",
        "    driver = setup_driver()\n",
        "\n",
        "    if not driver:\n",
        "        print(\"Driver initialization failed. Exiting...\")\n",
        "        return pd.DataFrame(articles)\n",
        "\n",
        "    for page in range(1, num_pages + 1):\n",
        "        try:\n",
        "            print(f\"Scraping page {page}...\")\n",
        "            driver.get(f\"{base_url}/page/{page}\")\n",
        "            time.sleep(3)  # Allow time for JS to load\n",
        "\n",
        "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "            news_items = soup.find_all(\"article\")  # Adjust based on site structure\n",
        "\n",
        "            if not news_items:\n",
        "                print(f\"No articles found on page {page}\")\n",
        "                continue\n",
        "\n",
        "            for item in news_items:\n",
        "                try:\n",
        "                    headline = item.find(\"h2\").text.strip() if item.find(\"h2\") else \"N/A\"\n",
        "                    author = item.find(class_=\"author\").text.strip() if item.find(class_=\"author\") else \"Unknown\"\n",
        "                    date = item.find(\"time\").text.strip() if item.find(\"time\") else \"Unknown\"\n",
        "                    content_link = item.find(\"a\")[\"href\"] if item.find(\"a\") else None\n",
        "                    category = item.find(class_=\"category\").text.strip() if item.find(class_=\"category\") else \"General\"\n",
        "\n",
        "                    if content_link:\n",
        "                        try:\n",
        "                            driver.get(content_link)\n",
        "                            time.sleep(2)\n",
        "                            article_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "                            content = article_soup.find(\"div\", class_=\"article-body\").text.strip() if article_soup.find(\"div\", class_=\"article-body\") else \"No content\"\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error fetching article content: {e}\")\n",
        "                            content = \"Error fetching content\"\n",
        "                    else:\n",
        "                        content = \"No content\"\n",
        "\n",
        "                    articles.append({\n",
        "                        \"Headline\": headline,\n",
        "                        \"Author\": author,\n",
        "                        \"Publication Date\": date,\n",
        "                        \"Category\": category,\n",
        "                        \"Content\": content\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing article: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error on page {page}: {e}\")\n",
        "\n",
        "    driver.quit()\n",
        "    return pd.DataFrame(articles)\n",
        "\n",
        "# Example usage\n",
        "news_url = \"https://www.bbc.com/news\"  # Change to the actual site\n",
        "news_data = scrape_articles(news_url, num_pages=3)\n",
        "\n",
        "# Save to CSV\n",
        "if not news_data.empty:\n",
        "    news_data.to_csv(\"news_articles.csv\", index=False)\n",
        "    print(\"Scraping completed. Data saved as news_articles.csv\")\n",
        "else:\n",
        "    print(\"No data scraped. Exiting...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrpzWoZrjZJa",
        "outputId": "6f17f6c5-0c15-49f9-ddd4-03d8a438890d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Scraping page 3...\n",
            "Scraping completed. Data saved as news_articles.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.bbc.com/news\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "response = requests.get(url, headers=headers)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "articles = []\n",
        "for item in soup.find_all(\"div\", class_=\"gs-c-promo\"):\n",
        "    headline = item.find(\"h3\").text if item.find(\"h3\") else None\n",
        "    link = item.find(\"a\")[\"href\"] if item.find(\"a\") else None\n",
        "    articles.append({\"headline\": headline, \"link\": link})\n",
        "\n",
        "print(articles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwf0a9n8oH2R",
        "outputId": "ecff14be-df37-42b0-c2a1-d2bd40d930f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Read the sample news CSV file\n",
        "df = pd.read_csv(\"/content/sample_data/sample_news.csv\", low_memory=False)\n",
        "\n",
        "# Print column names for verification\n",
        "print(\"Columns in dataset:\", df.columns)\n",
        "\n",
        "# Handle missing 'author' column values\n",
        "if 'author' not in df.columns:\n",
        "    df['author'] = \"Unknown\"\n",
        "else:\n",
        "    df[\"author\"].fillna(\"Unknown\", inplace=True)\n",
        "\n",
        "# Fix encoding for 'content' column\n",
        "if 'content' in df.columns:\n",
        "    df[\"content\"] = df[\"content\"].astype(str).apply(lambda x: x.encode(\"utf-8\", \"ignore\").decode(\"utf-8\"))\n",
        "else:\n",
        "    raise ValueError(\"Column 'content' not found. Please check your CSV file.\")\n",
        "\n",
        "# Function to extract keywords\n",
        "def extract_keywords(text):\n",
        "    words = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
        "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
        "    return Counter(words).most_common(10)\n",
        "\n",
        "# Apply keyword extraction\n",
        "df[\"keywords\"] = df[\"content\"].apply(extract_keywords)\n",
        "\n",
        "# Save cleaned data\n",
        "cleaned_csv_path = \"/content/sample_data/sample_news.csv\"\n",
        "df.to_csv(cleaned_csv_path, index=False)\n",
        "\n",
        "print(\"Data cleaning completed. File saved as 'cleaned_news.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oql9MBMSoZm9",
        "outputId": "665a1c5c-fabb-4c97-8fed-62bcfcca47b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in dataset: Index(['headline', 'author', 'publication_date', 'category', 'content',\n",
            "       'source_url'],\n",
            "      dtype='object')\n",
            "Data cleaning completed. File saved as 'cleaned_news.csv'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<ipython-input-13-64fcfc39134a>:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[\"author\"].fillna(\"Unknown\", inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect(\"news.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Create table\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS articles (\n",
        "    id INTEGER PRIMARY KEY,\n",
        "    headline TEXT,\n",
        "    author TEXT,\n",
        "    publication_date TEXT,\n",
        "    category TEXT,\n",
        "    content TEXT,\n",
        "    source_url TEXT\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "# Insert data\n",
        "df = pd.read_csv(\"/content/sample_data/sample_news.csv\")\n",
        "df.to_sql(\"articles\", conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "XhrU5RfupgwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10)\n",
        "tfidf_matrix = vectorizer.fit_transform(df[\"content\"])\n",
        "keywords = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"Top 10 Keywords:\", keywords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9r669BzpoFo",
        "outputId": "e832659f-c233-494a-ea54-60d54e455cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Keywords: ['ai' 'major' 'markets' 'medical' 'nation' 'positive' 'rise' 'scientists'\n",
            " 'sparked' 'stock']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    return [ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"GPE\", \"PERSON\"]]\n",
        "\n",
        "df[\"entities\"] = df[\"content\"].apply(extract_entities)\n"
      ],
      "metadata": {
        "id": "IT8iGNm4puOd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}